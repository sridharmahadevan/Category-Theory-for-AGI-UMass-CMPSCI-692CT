\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{default}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{booktabs}

\title{Lecture 9: Information Flow in GT/DB}
\subtitle{Causality, Side Information, and Fair Comparison}
\author{CMPSCI 692CT}
\date{}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\begin{frame}{Motivation}
\begin{itemize}
  \item Student question: ``Is GT cheating on language modeling by getting extra information?''
  \item We tested this directly with controlled ablations on PTB.
  \item Key issue: \textbf{architecture gain} vs \textbf{information-access gain}.
\end{itemize}
\end{frame}

\begin{frame}{What Counts as Fair?}
\begin{itemize}
  \item \textbf{Fair-causal regime}: output at time $t$ depends only on tokens $\le t$.
  \item \textbf{Augmented-context regime}: model gets side information (future-informative hints).
  \item Inductive bias is valid if information budget is declared and matched across baselines.
\end{itemize}
\end{frame}

\begin{frame}{Why Leakage Happens}
\begin{itemize}
  \item Teacher forcing feeds shifted ground-truth targets into decoder.
  \item Non-causal local mixing can pass future-position representations backward.
  \item Even if raw next token is not explicitly injected, future-informative features can leak.
\end{itemize}
\vspace{0.4em}
\[
h_t \xleftarrow{\text{non-causal mix}} h_{t+1},\quad
h_{t+1} \text{ contains information about target at } t.
\]
\end{frame}

\begin{frame}{Models Compared}
\begin{itemize}
  \item \texttt{transformer\_causal}
  \item \texttt{gt\_causal}
  \item \texttt{gt\_noncausal}
  \item \texttt{transformer\_future\_hint}
  \item \texttt{gt\_pred\_next\_detach}
  \item \texttt{gt\_pred\_prev\_causal\_detach} (strict shifted self-conditioning)
\end{itemize}
\end{frame}

\begin{frame}{Validation Loss}
  \centering
  \includegraphics[width=0.95\linewidth]{lecture9_val_loss.png}
\end{frame}

\begin{frame}{Validation Perplexity}
  \centering
  \includegraphics[width=0.95\linewidth]{lecture9_val_ppl.png}
\end{frame}

\begin{frame}{Interpretation}
\begin{itemize}
  \item \texttt{gt\_noncausal} and \texttt{transformer\_future\_hint} collapse to near-1 perplexity.
  \item \texttt{gt\_pred\_next\_detach} also collapses: detached prediction can still carry future-informative structure.
  \item \texttt{transformer\_causal}, \texttt{gt\_causal}, and \texttt{gt\_pred\_prev\_causal\_detach} track each other.
\end{itemize}
\vspace{0.4em}
\textbf{Conclusion:} large gains came from information regime, not from GT architecture alone.
\end{frame}

\begin{frame}{What We Showed}
\begin{itemize}
  \item We separated ``better model'' from ``more information''.
  \item Strict causal self-conditioning is possible:
  \[
  \tilde{e}_t = \hat{e}_{t-1}, \quad \hat{e}_{t-1}=f_\theta(x_{\le t-1})
  \]
  \item This avoids direct future leakage but does not produce artificial collapse.
\end{itemize}
\end{frame}

\begin{frame}{Category-Theoretic Reading}
\begin{itemize}
  \item Time indices form a directed poset category.
  \item Causal model: morphisms respect time direction.
  \item Non-causal mixing adds backward arrows (future $\to$ past), changing the information category.
  \item GT/DB claims must specify which category (causal vs augmented) is being optimized.
\end{itemize}
\end{frame}

\begin{frame}{Experimental Protocol for Projects}
\begin{itemize}
  \item Always report regime: \texttt{fair-causal} vs \texttt{augmented-context}.
  \item Always log flags: \texttt{geo\_causal}, hint source, and side-channel settings.
  \item Compare against matched side-information baselines.
  \item Include leakage sensitivity test in appendices.
\end{itemize}
\end{frame}

\begin{frame}{Takeaway}
\begin{itemize}
  \item GT/DB remains useful.
  \item But claims must be tied to the information budget.
  \item Better science: explicit regimes, matched controls, and causal audits.
\end{itemize}
\end{frame}

\end{document}
