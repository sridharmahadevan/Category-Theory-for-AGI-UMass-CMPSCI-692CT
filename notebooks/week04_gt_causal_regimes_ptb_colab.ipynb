{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 â€” Standalone GT Causal vs Augmented Regimes (PTB / WikiText-2)\n",
    "\n",
    "This notebook is **standalone** and runs directly in Colab.\n",
    "No Hugging Face login/token is required.\\n",
    "\n",
    "It compares six variants under a shared training budget:\n",
    "- `transformer_causal`\n",
    "- `transformer_future_hint`\n",
    "- `gt_causal`\n",
    "- `gt_noncausal`\n",
    "- `gt_pred_next_detach`\n",
    "- `gt_pred_prev_causal_detach`\n",
    "\n",
    "Recommended workflow:\n",
    "1. Start with `DATASET='ptb'`.\n",
    "2. Then try `DATASET='wikitext2'`.\n",
    "3. Compare strict-causal vs augmented-context vs transition behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies (Colab)\n",
    "!python -m pip install -q --upgrade pip\n",
    "!python -m pip install -q matplotlib pandas numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports and device setup\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "from urllib import request\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def set_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Config\n",
    "DATASET = 'ptb'  #@param ['ptb', 'wikitext2']\n",
    "MAX_VOCAB = 10000 #@param {type:'integer'}\n",
    "CONTEXT = 128 #@param {type:'integer'}\n",
    "D_MODEL = 64 #@param {type:'integer'}\n",
    "N_LAYERS = 2 #@param {type:'integer'}\n",
    "N_HEADS = 4 #@param {type:'integer'}\n",
    "BATCH_SIZE = 24 #@param {type:'integer'}\n",
    "NUM_ITERS = 5000 #@param {type:'integer'}\n",
    "EVAL_EVERY = 500 #@param {type:'integer'}\n",
    "LR = 3e-4 #@param {type:'number'}\n",
    "WEIGHT_DECAY = 1e-5 #@param {type:'number'}\n",
    "HINT_SCALE = 0.2 #@param {type:'number'}\n",
    "PRED_TEMP = 1.0 #@param {type:'number'}\n",
    "\n",
    "print(dict(DATASET=DATASET, CONTEXT=CONTEXT, D_MODEL=D_MODEL, N_LAYERS=N_LAYERS, NUM_ITERS=NUM_ITERS))\n",
    "\n",
    "# Optional preset for stronger transition effects on PTB\n",
    "PHASE_TRANSITION_PRESET = True  #@param {type:'boolean'}\n",
    "if PHASE_TRANSITION_PRESET and DATASET == 'ptb':\n",
    "    CONTEXT = max(CONTEXT, 128)\n",
    "    NUM_ITERS = max(NUM_ITERS, 5000)\n",
    "    EVAL_EVERY = max(EVAL_EVERY, 500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on phase-transition behavior**\\n",
    "For `gt_pred_next_detach`, transition-like behavior usually appears later than 1.2k steps.\\n",
    "Recommended PTB setting: `CONTEXT>=128`, `NUM_ITERS>=5000`, and optionally 2-3 seeds for stability.\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Load PTB/WikiText-2 from direct URLs (no HF token needed)\n",
    "print('No Hugging Face datasets dependency: using direct URL text files.')\n",
    "def download_ptb(save_dir='/tmp/ptb'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    base = 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/'\n",
    "    files = ['ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt']\n",
    "    out = []\n",
    "    for fn in files:\n",
    "        path = os.path.join(save_dir, fn)\n",
    "        if not os.path.exists(path):\n",
    "            request.urlretrieve(base + fn, path)\n",
    "        out.append(path)\n",
    "    return out\n",
    "\n",
    "def download_wikitext2(save_dir='/tmp/wikitext2'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    base = 'https://raw.githubusercontent.com/pytorch/examples/master/word_language_model/data/wikitext-2/'\n",
    "    files = ['train.txt', 'valid.txt', 'test.txt']\n",
    "    out = []\n",
    "    for fn in files:\n",
    "        path = os.path.join(save_dir, fn)\n",
    "        if not os.path.exists(path):\n",
    "            request.urlretrieve(base + fn, path)\n",
    "        out.append(path)\n",
    "    return out\n",
    "\n",
    "def load_tokens(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        lines = [line.strip().split() for line in f]\n",
    "    return [tok for line in lines for tok in (line + ['<eos>'])]\n",
    "\n",
    "def get_token_splits(name):\n",
    "    if name == 'ptb':\n",
    "        tr, va, te = download_ptb()\n",
    "    elif name == 'wikitext2':\n",
    "        tr, va, te = download_wikitext2()\n",
    "    else:\n",
    "        raise ValueError(name)\n",
    "    return load_tokens(tr), load_tokens(va), load_tokens(te)\n",
    "\n",
    "def build_vocab(train_tokens, max_vocab=10000):\n",
    "    cnt = Counter(train_tokens)\n",
    "    vocab = ['<pad>', '<unk>'] + [w for w,_ in cnt.most_common(max_vocab-2)]\n",
    "    stoi = {w:i for i,w in enumerate(vocab)}\n",
    "    return vocab, stoi\n",
    "\n",
    "def to_ids(tokens, stoi):\n",
    "    unk = stoi['<unk>']\n",
    "    return np.array([stoi.get(w, unk) for w in tokens], dtype=np.int64)\n",
    "\n",
    "train_tokens, valid_tokens, test_tokens = get_token_splits(DATASET)\n",
    "vocab, stoi = build_vocab(train_tokens, MAX_VOCAB)\n",
    "train_np = to_ids(train_tokens, stoi)\n",
    "valid_np = to_ids(valid_tokens, stoi)\n",
    "test_np = to_ids(test_tokens, stoi)\n",
    "vocab_size = len(vocab)\n",
    "print('vocab_size:', vocab_size, 'train_tokens:', len(train_np), 'valid_tokens:', len(valid_np), 'test_tokens:', len(test_np))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Models (Transformer + GT-Lite variants)\n",
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, max_len=2048, future_hint=False, hint_scale=0.2):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(max_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=d_model, nhead=n_heads, dim_feedforward=4*d_model, batch_first=True)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        self.future_hint = future_hint\n",
    "        self.hint_scale = float(hint_scale)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        h = self.emb(x) + self.pos(pos)\n",
    "        mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        for blk in self.blocks:\n",
    "            h = blk(h, src_mask=mask)\n",
    "\n",
    "        if self.future_hint and L > 1:\n",
    "            # Explicit non-causal local hint control\n",
    "            h[:, :-1, :] = h[:, :-1, :] + self.hint_scale * h[:, 1:, :]\n",
    "\n",
    "        return self.out(h)\n",
    "\n",
    "class GeometricTransformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, geo_causal=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.GELU(), nn.Linear(4*d_model, d_model))\n",
    "        self.geo_causal = bool(geo_causal)\n",
    "        self.geo_conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=0 if geo_causal else 1, groups=d_model)\n",
    "        self.n1 = nn.LayerNorm(d_model)\n",
    "        self.n2 = nn.LayerNorm(d_model)\n",
    "        self.ng = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, h, mask, geo_in=None):\n",
    "        a,_ = self.attn(h,h,h,attn_mask=mask,need_weights=False)\n",
    "        h = self.n1(h + self.drop(a))\n",
    "        h = self.n2(h + self.drop(self.ffn(h)))\n",
    "\n",
    "        gbase = h if geo_in is None else geo_in\n",
    "        g = gbase.transpose(1,2)\n",
    "        if self.geo_causal:\n",
    "            g = F.pad(g, (2,0))\n",
    "        g = self.geo_conv(g).transpose(1,2)\n",
    "        h = self.ng(h + self.drop(g))\n",
    "        return h\n",
    "\n",
    "class GTLiteLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, max_len=2048, geo_causal=False, geo_mix_source='hidden', pred_temp=1.0):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos = nn.Embedding(max_len, d_model)\n",
    "        self.layers = nn.ModuleList([GeometricTransformerBlock(d_model, n_heads, geo_causal=geo_causal) for _ in range(n_layers)])\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "        self.geo_mix_source = geo_mix_source\n",
    "        self.pred_temp = float(pred_temp)\n",
    "\n",
    "    def _pred_emb(self, h, detach=True):\n",
    "        logits = self.out(h) / max(1e-6, self.pred_temp)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        pe = probs @ self.emb.weight\n",
    "        return pe.detach() if detach else pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, L = x.shape\n",
    "        pos = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
    "        h = self.emb(x) + self.pos(pos)\n",
    "        mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), diagonal=1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            geo_in = None\n",
    "            if self.geo_mix_source == 'pred_next_detach':\n",
    "                geo_in = self._pred_emb(h, detach=True)\n",
    "            elif self.geo_mix_source == 'pred_prev_causal_detach':\n",
    "                pe = self._pred_emb(h, detach=True)\n",
    "                geo_in = torch.zeros_like(pe)\n",
    "                geo_in[:, 1:, :] = pe[:, :-1, :]\n",
    "            h = layer(h, mask, geo_in=geo_in)\n",
    "\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Training + evaluation utilities\n",
    "def sample_windows(arr, batch_size, context):\n",
    "    n = len(arr) - context - 1\n",
    "    ids = np.random.randint(0, n, size=batch_size)\n",
    "    x = np.stack([arr[i:i+context] for i in ids])\n",
    "    y = np.stack([arr[i+1:i+1+context] for i in ids])\n",
    "    return torch.from_numpy(x).long(), torch.from_numpy(y).long()\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_model(model, arr, batch_size=32, context=64, num_batches=30):\n",
    "    model.eval()\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    total_loss = 0.0\n",
    "    total_tok = 0\n",
    "    for _ in range(num_batches):\n",
    "        x, y = sample_windows(arr, batch_size, context)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        loss = ce(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "        total_loss += float(loss.item()) * y.numel()\n",
    "        total_tok += y.numel()\n",
    "    avg = total_loss / max(1, total_tok)\n",
    "    return avg, math.exp(avg)\n",
    "\n",
    "def run_variant(name, model):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    hist = []\n",
    "    model.train()\n",
    "    for it in range(1, NUM_ITERS + 1):\n",
    "        x, y = sample_windows(train_np, BATCH_SIZE, CONTEXT)\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = ce(logits.reshape(-1, logits.size(-1)), y.reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        opt.step()\n",
    "\n",
    "        if it % EVAL_EVERY == 0 or it == NUM_ITERS:\n",
    "            vloss, vppl = eval_model(model, valid_np, batch_size=BATCH_SIZE, context=CONTEXT, num_batches=25)\n",
    "            hist.append({'iter': it, 'val_loss': vloss, 'val_ppl': vppl})\n",
    "            print(f'[{name}] iter={it} val_loss={vloss:.3f} val_ppl={vppl:.3f}')\n",
    "\n",
    "    tloss, tppl = eval_model(model, test_np, batch_size=BATCH_SIZE, context=CONTEXT, num_batches=40)\n",
    "    return hist, tloss, tppl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run six-way causal-ablation comparison\n",
    "set_seed(0)\n",
    "\n",
    "variants = {\n",
    "    'transformer_causal': lambda: TransformerLM(vocab_size, D_MODEL, N_LAYERS, N_HEADS, max_len=CONTEXT, future_hint=False, hint_scale=HINT_SCALE),\n",
    "    'transformer_future_hint': lambda: TransformerLM(vocab_size, D_MODEL, N_LAYERS, N_HEADS, max_len=CONTEXT, future_hint=True, hint_scale=HINT_SCALE),\n",
    "    'gt_noncausal': lambda: GTLiteLM(vocab_size, D_MODEL, N_LAYERS, N_HEADS, max_len=CONTEXT, geo_causal=False, geo_mix_source='hidden', pred_temp=PRED_TEMP),\n",
    "    'gt_causal': lambda: GTLiteLM(vocab_size, D_MODEL, N_LAYERS, N_HEADS, max_len=CONTEXT, geo_causal=True, geo_mix_source='hidden', pred_temp=PRED_TEMP),\n",
    "    'gt_pred_next_detach': lambda: GTLiteLM(vocab_size, D_MODEL, N_LAYERS, N_HEADS, max_len=CONTEXT, geo_causal=False, geo_mix_source='pred_next_detach', pred_temp=PRED_TEMP),\n",
    "    'gt_pred_prev_causal_detach': lambda: GTLiteLM(vocab_size, D_MODEL, N_LAYERS, N_HEADS, max_len=CONTEXT, geo_causal=True, geo_mix_source='pred_prev_causal_detach', pred_temp=PRED_TEMP),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "rows = []\n",
    "for name, ctor in variants.items():\n",
    "    print(f'\\n=== {name} ===')\n",
    "    model = ctor().to(device)\n",
    "    hist, test_loss, test_ppl = run_variant(name, model)\n",
    "    results[name] = {'history': hist, 'test_loss': test_loss, 'test_ppl': test_ppl}\n",
    "    last = hist[-1] if hist else {'val_loss': float('nan'), 'val_ppl': float('nan')}\n",
    "    rows.append({\n",
    "        'model': name,\n",
    "        'last_val_loss': last['val_loss'],\n",
    "        'last_val_ppl': last['val_ppl'],\n",
    "        'test_loss': test_loss,\n",
    "        'test_ppl': test_ppl,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "display(df.sort_values('test_ppl'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Plot validation loss and perplexity curves\n",
    "plt.figure(figsize=(9,4))\n",
    "for name, obj in results.items():\n",
    "    h = obj['history']\n",
    "    if not h:\n",
    "        continue\n",
    "    xs = [r['iter'] for r in h]\n",
    "    ys = [r['val_loss'] for r in h]\n",
    "    plt.plot(xs, ys, 'o--', label=name)\n",
    "plt.title(f'Val loss ({DATASET})')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Val loss')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.02,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(9,4))\n",
    "for name, obj in results.items():\n",
    "    h = obj['history']\n",
    "    if not h:\n",
    "        continue\n",
    "    xs = [r['iter'] for r in h]\n",
    "    ys = [r['val_ppl'] for r in h]\n",
    "    plt.plot(xs, ys, 'o--', label=name)\n",
    "plt.title(f'Val perplexity ({DATASET})')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Val ppl')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(bbox_to_anchor=(1.02,1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Auto-interpretation summary\n",
    "def regime(m):\n",
    "    if m in ['transformer_causal', 'gt_causal', 'gt_pred_prev_causal_detach']:\n",
    "        return 'C'\n",
    "    if m in ['transformer_future_hint', 'gt_noncausal']:\n",
    "        return 'A'\n",
    "    if m == 'gt_pred_next_detach':\n",
    "        return 'Transition'\n",
    "    return '?'\n",
    "\n",
    "df['regime'] = df['model'].map(regime)\n",
    "display(df[['model','regime','last_val_ppl','test_ppl']].sort_values('test_ppl'))\n",
    "\n",
    "m = {r['model']: float(r['test_ppl']) for _, r in df.iterrows()}\n",
    "causal_best = min(m['transformer_causal'], m['gt_causal'], m['gt_pred_prev_causal_detach'])\n",
    "causal_worst = max(m['transformer_causal'], m['gt_causal'], m['gt_pred_prev_causal_detach'])\n",
    "aug_best = min(m['transformer_future_hint'], m['gt_noncausal'])\n",
    "aug_worst = max(m['transformer_future_hint'], m['gt_noncausal'])\n",
    "pred_next = m['gt_pred_next_detach']\n",
    "gain = (causal_best - pred_next) / (causal_best - aug_best + 1e-12)\n",
    "\n",
    "print(f'Dataset: {DATASET}')\n",
    "print(f'Causal cluster test PPL range: [{causal_best:.3f}, {causal_worst:.3f}]')\n",
    "print(f'Augmented cluster test PPL range: [{aug_best:.3f}, {aug_worst:.3f}]')\n",
    "print(f'gt_pred_next_detach test PPL: {pred_next:.3f}')\n",
    "print(f'transition_gain: {gain:.3f}')\n",
    "\n",
    "if pred_next <= aug_worst:\n",
    "    print('Interpretation: transition variant is in/near augmented regime for this run.')\n",
    "elif pred_next < causal_best:\n",
    "    print('Interpretation: transition variant is intermediate (better than strict-causal, not yet augmented).')\n",
    "else:\n",
    "    print('Interpretation: transition variant remains strict-causal-like for this run.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting the Result (Student Notes)\n",
    "- In this run, `gt_pred_next_detach` starts near the strict-causal models, then drops sharply after ~2k-2.5k steps.\n",
    "- This is the **phase-transition pattern**: prediction-based hints become useful enough to move performance toward the augmented regime.\n",
    "- `gt_noncausal` remains the upper bound here because it has the strongest augmented-context pathway.\n",
    "- `gt_pred_prev_causal_detach` staying near causal baselines is an important control: strict causal self-conditioning alone does not create the same jump.\n",
    "- Practical lesson: the transition is sensitive to training horizon and context length (typically clearer with larger `CONTEXT` and longer runs).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}