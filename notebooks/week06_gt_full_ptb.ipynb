{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 6 \u2014 GT\u2011Full on PTB (Tiny Language Model)\n",
        "We compare a small **Transformer LM** against a **GT\u2011Full LM** that uses relation\u2011aware message passing on the sequence graph.\n",
        "This is intentionally tiny for classroom speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "from urllib import request\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load PTB (word-level)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_ptb(save_dir='/tmp/ptb'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    base = 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/'\n",
        "    files = ['ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt']\n",
        "    for fn in files:\n",
        "        path = os.path.join(save_dir, fn)\n",
        "        if not os.path.exists(path):\n",
        "            request.urlretrieve(base + fn, path)\n",
        "    return [os.path.join(save_dir, f) for f in files]\n",
        "\n",
        "def load_tokens(path):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = [line.strip().split() for line in f]\n",
        "    return [tok for line in lines for tok in (line + ['<eos>'])]\n",
        "\n",
        "train_path, valid_path, test_path = download_ptb()\n",
        "train_tokens = load_tokens(train_path)\n",
        "valid_tokens = load_tokens(valid_path)\n",
        "\n",
        "vocab = {w:i for i,w in enumerate(sorted(set(train_tokens)))}\n",
        "def to_ids(tokens):\n",
        "    return torch.tensor([vocab[w] for w in tokens], dtype=torch.long)\n",
        "\n",
        "train_ids = to_ids(train_tokens)\n",
        "valid_ids = to_ids(valid_tokens)\n",
        "vocab_size = len(vocab)\n",
        "print('Vocab size:', vocab_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Batching utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batchify(data, batch_size):\n",
        "    n_batch = data.size(0) // batch_size\n",
        "    data = data[:n_batch * batch_size]\n",
        "    data = data.view(batch_size, -1).t().contiguous()  # (T, B)\n",
        "    return data\n",
        "\n",
        "def get_batch(source, i, bptt=20):\n",
        "    seq_len = min(bptt, source.size(0) - 1 - i)\n",
        "    data = source[i:i+seq_len]\n",
        "    target = source[i+1:i+1+seq_len]\n",
        "    return data, target\n",
        "\n",
        "batch_size = 20\n",
        "bptt = 20\n",
        "train_data = batchify(train_ids, batch_size)\n",
        "valid_data = batchify(valid_ids, batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe)\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)].unsqueeze(1)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dim_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(nn.Linear(d_model, dim_ff), nn.ReLU(), nn.Dropout(dropout), nn.Linear(dim_ff, d_model))\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
        "        x = self.ln1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class SimplicialMessagePassing(nn.Module):\n",
        "    def __init__(self, dim, num_rel, hidden_dim=None):\n",
        "        super().__init__()\n",
        "        hidden_dim = hidden_dim or dim\n",
        "        self.edge_mlp = nn.Sequential(nn.Linear(2*dim + num_rel, hidden_dim), nn.ReLU(), nn.Linear(hidden_dim, dim))\n",
        "    def forward(self, V, edge_index, rel_ids):\n",
        "        src = edge_index[0].long()\n",
        "        dst = edge_index[1].long()\n",
        "        src_h = V[src]\n",
        "        dst_h = V[dst]\n",
        "        num_rel = int(rel_ids.max().item()) + 1 if rel_ids.numel() > 0 else 0\n",
        "        rel_onehot = F.one_hot(rel_ids.long(), num_classes=num_rel).float()\n",
        "        edge_feat = torch.cat([src_h, dst_h, rel_onehot], dim=-1)\n",
        "        msg = self.edge_mlp(edge_feat)\n",
        "        out = torch.zeros_like(V)\n",
        "        out.index_add_(0, dst, msg)\n",
        "        return V + out\n",
        "\n",
        "class GTFullLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=96, n_layers=2, num_rel=3):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([SimplicialMessagePassing(d_model, num_rel) for _ in range(n_layers)])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "    def forward(self, x, edge_index, rel_ids):\n",
        "        emb = self.tok(x)\n",
        "        h = self.pos(emb)\n",
        "        T, B, D = h.shape\n",
        "        h = h.reshape(T*B, D)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, edge_index, rel_ids)\n",
        "        h = self.ln(h).reshape(T, B, D)\n",
        "        return self.out(h)\n",
        "\n",
        "class TransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, n_heads=2, n_layers=1, dim_ff=128):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        self.layers = nn.ModuleList([TransformerBlock(d_model, n_heads, dim_ff) for _ in range(n_layers)])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "    def _causal_mask(self, T, device):\n",
        "        mask = torch.triu(torch.full((T, T), float('-inf'), device=device), diagonal=1)\n",
        "        return mask\n",
        "    def forward(self, x):\n",
        "        emb = self.tok(x)\n",
        "        h = self.pos(emb)\n",
        "        mask = self._causal_mask(x.size(0), x.device)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, attn_mask=mask)\n",
        "        h = self.ln(h)\n",
        "        return self.out(h)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Training / evaluation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def build_edges(T, B):\n",
        "    # edges between adjacent positions (rel 0/1) and skip-2 (rel 2)\n",
        "    src = []\n",
        "    dst = []\n",
        "    rel = []\n",
        "    for b in range(B):\n",
        "        offset = b * T\n",
        "        for t in range(T-1):\n",
        "            i = offset + t\n",
        "            j = offset + t + 1\n",
        "            src.append(i); dst.append(j); rel.append(0)\n",
        "            src.append(j); dst.append(i); rel.append(1)\n",
        "        for t in range(T-2):\n",
        "            i = offset + t\n",
        "            j = offset + t + 2\n",
        "            src.append(i); dst.append(j); rel.append(2)\n",
        "    edge_index = torch.tensor([src, dst], dtype=torch.long, device=device)\n",
        "    rel_ids = torch.tensor(rel, dtype=torch.long, device=device)\n",
        "    return edge_index, rel_ids\n",
        "\n",
        "def evaluate(model, data_source, is_gt=False):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i, bptt)\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            if is_gt:\n",
        "                edge_index, rel_ids = build_edges(data.size(0), data.size(1))\n",
        "                logits = model(data, edge_index, rel_ids)\n",
        "            else:\n",
        "                logits = model(data)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.reshape(-1))\n",
        "            total_loss += loss.item() * targets.numel()\n",
        "            total_tokens += targets.numel()\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n",
        "\n",
        "def train_model(model, name, train_data, valid_data, is_gt=False, max_steps=300):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "    val_ppl = []\n",
        "    step = 0\n",
        "    model.train()\n",
        "    for epoch in range(1000):\n",
        "        for i in range(0, train_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(train_data, i, bptt)\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            if is_gt:\n",
        "                edge_index, rel_ids = build_edges(data.size(0), data.size(1))\n",
        "                logits = model(data, edge_index, rel_ids)\n",
        "            else:\n",
        "                logits = model(data)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.reshape(-1))\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            step += 1\n",
        "            if step % 50 == 0:\n",
        "                v_loss, v_ppl = evaluate(model, valid_data, is_gt=is_gt)\n",
        "                val_ppl.append(v_ppl)\n",
        "                print(f'[{name}] step {step:4d} | val_ppl={v_ppl:.2f}')\n",
        "            if step >= max_steps:\n",
        "                return val_ppl\n",
        "\n",
        "# Train\n",
        "tf_model = TransformerLM(vocab_size)\n",
        "gt_model = GTFullLM(vocab_size)\n",
        "\n",
        "tf_ppl = train_model(tf_model, 'Transformer', train_data, valid_data, is_gt=False, max_steps=300)\n",
        "gt_ppl = train_model(gt_model, 'GT\u2011Full', train_data, valid_data, is_gt=True, max_steps=300)\n",
        "\n",
        "import numpy as np\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(np.minimum.accumulate(tf_ppl), label='Transformer (best-so-far)')\n",
        "plt.plot(np.minimum.accumulate(gt_ppl), label='GT\u2011Full (best-so-far)')\n",
        "plt.xlabel('Eval checkpoints')\n",
        "plt.ylabel('Validation perplexity')\n",
        "plt.title('GT\u2011Full vs Transformer (PTB, tiny)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "week06_gt_full_ptb.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}