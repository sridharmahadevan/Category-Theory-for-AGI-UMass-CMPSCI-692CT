{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Geometric Transformer + Diagrammatic Backprop (Sudoku 4x4)\n",
        "This notebook is a gentle, end-to-end example for the **Category Theory for AGI** course.\n",
        "We solve tiny 4x4 Sudoku puzzles with two models:\n",
        "- **Baseline Transformer**\n",
        "- **Geometric Transformer (GT-Lite)** with an optional **diagrammatic backprop** penalty\n",
        "\n",
        "The DB signal is implemented as a **triangle consistency** loss: for each triangle in the Sudoku constraint graph, we encourage the three embeddings to agree with their mean.\n",
        "This is a simple, local proxy for \u201cdiagrammatic curvature\u201d without introducing heavier topology machinery.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import math\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_device():\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    return torch.device('cpu')\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(0)\n",
        "device = get_device()\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Tiny 4x4 Sudoku dataset\n",
        "We generate many valid 4x4 Sudoku solutions by permuting a base solution, then mask some entries.\n",
        "Digits are encoded as `0..3`; blanks are `-1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class SudokuInstance:\n",
        "    puzzle: torch.Tensor   # (16,) digits 0..3, or -1 for blank\n",
        "    solution: torch.Tensor # (16,) digits 0..3\n",
        "\n",
        "def base_solution_matrix():\n",
        "    return torch.tensor([\n",
        "        [0, 1, 2, 3],\n",
        "        [2, 3, 0, 1],\n",
        "        [1, 0, 3, 2],\n",
        "        [3, 2, 1, 0],\n",
        "    ], dtype=torch.long)\n",
        "\n",
        "def random_sudoku_4x4_solution() -> torch.Tensor:\n",
        "    M = base_solution_matrix()\n",
        "    # Permute digits\n",
        "    digit_perm = torch.randperm(4)\n",
        "    M = digit_perm[M]\n",
        "\n",
        "    # Permute rows within bands and swap bands\n",
        "    band_rows = [[0, 1], [2, 3]]\n",
        "    permuted_rows = []\n",
        "    for band in band_rows:\n",
        "        order = band.copy()\n",
        "        random.shuffle(order)\n",
        "        permuted_rows.extend(order)\n",
        "    M = M[permuted_rows, :]\n",
        "    if random.random() < 0.5:\n",
        "        M = torch.cat([M[2:4, :], M[0:2, :]], dim=0)\n",
        "\n",
        "    # Permute cols within bands and swap bands\n",
        "    band_cols = [[0, 1], [2, 3]]\n",
        "    permuted_cols = []\n",
        "    for band in band_cols:\n",
        "        order = band.copy()\n",
        "        random.shuffle(order)\n",
        "        permuted_cols.extend(order)\n",
        "    M = M[:, permuted_cols]\n",
        "    if random.random() < 0.5:\n",
        "        M = torch.cat([M[:, 2:4], M[:, 0:2]], dim=1)\n",
        "\n",
        "    return M.reshape(-1)  # (16,)\n",
        "\n",
        "def mask_puzzle(solution: List[int], num_givens: int) -> List[int]:\n",
        "    idxs = list(range(16))\n",
        "    random.shuffle(idxs)\n",
        "    given_idxs = set(idxs[:num_givens])\n",
        "    puzzle = []\n",
        "    for i, v in enumerate(solution):\n",
        "        puzzle.append(v if i in given_idxs else -1)\n",
        "    return puzzle\n",
        "\n",
        "def make_sudoku_dataset(num_samples: int, num_givens: int) -> List[SudokuInstance]:\n",
        "    ds = []\n",
        "    for _ in range(num_samples):\n",
        "        sol_vec = random_sudoku_4x4_solution()\n",
        "        puzzle_vec = mask_puzzle(sol_vec.tolist(), num_givens)\n",
        "        ds.append(SudokuInstance(\n",
        "            puzzle=torch.tensor(puzzle_vec, dtype=torch.long),\n",
        "            solution=sol_vec.clone(),\n",
        "        ))\n",
        "    return ds\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Geometric Transformer blocks\n",
        "We use a lightweight GT block: self-attention + a local convolution path.\n",
        "The baseline Transformer omits the convolutional path.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GeomTransLiteBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, conv_kernel: int = 3, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.attn_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * d_model, d_model),\n",
        "        )\n",
        "        self.mlp_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.conv = nn.Conv1d(\n",
        "            d_model, d_model, kernel_size=conv_kernel, padding=conv_kernel // 2\n",
        "        )\n",
        "        self.conv_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.attn_norm(x + self.dropout(attn_out))\n",
        "\n",
        "        z = self.conv(x.transpose(1, 2)).transpose(1, 2)\n",
        "        x = self.conv_norm(x + self.dropout(z))\n",
        "\n",
        "        mlp_out = self.mlp(x)\n",
        "        x = self.mlp_norm(x + self.dropout(mlp_out))\n",
        "        return x\n",
        "\n",
        "class GTReasoner(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, num_layers: int):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            GeomTransLiteBlock(d_model, n_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "class PlainTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, batch_first=True)\n",
        "        self.attn_norm = nn.LayerNorm(d_model)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(d_model, 4 * d_model),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * d_model, d_model),\n",
        "        )\n",
        "        self.mlp_norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.attn_norm(x + self.dropout(attn_out))\n",
        "        mlp_out = self.mlp(x)\n",
        "        x = self.mlp_norm(x + self.dropout(mlp_out))\n",
        "        return x\n",
        "\n",
        "class PlainReasoner(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, num_layers: int):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            PlainTransformerBlock(d_model, n_heads) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        return self.norm(x)\n",
        "\n",
        "class SudokuGT(nn.Module):\n",
        "    def __init__(self, d_model=64, n_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.in_proj = nn.Linear(4 + 1, d_model)\n",
        "        self.reasoner = GTReasoner(d_model, n_heads, num_layers)\n",
        "        self.out_head = nn.Linear(d_model, 4)\n",
        "\n",
        "    def forward(self, puzzle: torch.Tensor, return_embeddings: bool = False):\n",
        "        if puzzle.ndim == 1:\n",
        "            puzzle = puzzle.unsqueeze(0)\n",
        "            squeeze_back = True\n",
        "        else:\n",
        "            squeeze_back = False\n",
        "\n",
        "        digits = puzzle.clone()\n",
        "        digits[digits == -1] = 0\n",
        "        digit_oh = F.one_hot(digits, num_classes=4).float()\n",
        "        given = (puzzle != -1).float().unsqueeze(-1)\n",
        "        x_feats = torch.cat([digit_oh, given], dim=-1)\n",
        "\n",
        "        h0 = self.in_proj(x_feats)\n",
        "        hT = self.reasoner(h0)\n",
        "        logits = self.out_head(hT)\n",
        "\n",
        "        if squeeze_back:\n",
        "            logits = logits.squeeze(0)\n",
        "            hT = hT.squeeze(0)\n",
        "\n",
        "        if return_embeddings:\n",
        "            return logits, hT\n",
        "        return logits\n",
        "\n",
        "class SudokuTransformer(nn.Module):\n",
        "    def __init__(self, d_model=64, n_heads=4, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.in_proj = nn.Linear(4 + 1, d_model)\n",
        "        self.reasoner = PlainReasoner(d_model, n_heads, num_layers)\n",
        "        self.out_head = nn.Linear(d_model, 4)\n",
        "\n",
        "    def forward(self, puzzle: torch.Tensor):\n",
        "        if puzzle.ndim == 1:\n",
        "            puzzle = puzzle.unsqueeze(0)\n",
        "            squeeze_back = True\n",
        "        else:\n",
        "            squeeze_back = False\n",
        "\n",
        "        digits = puzzle.clone()\n",
        "        digits[digits == -1] = 0\n",
        "        digit_oh = F.one_hot(digits, num_classes=4).float()\n",
        "        given = (puzzle != -1).float().unsqueeze(-1)\n",
        "        x_feats = torch.cat([digit_oh, given], dim=-1)\n",
        "\n",
        "        h0 = self.in_proj(x_feats)\n",
        "        hT = self.reasoner(h0)\n",
        "        logits = self.out_head(hT)\n",
        "\n",
        "        if squeeze_back:\n",
        "            logits = logits.squeeze(0)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Diagrammatic backprop (triangle consistency)\n",
        "We build triangles from Sudoku constraints (row, column, block).\n",
        "Each triangle\u2019s embeddings are encouraged to agree with their mean.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import itertools\n",
        "\n",
        "def build_sudoku_triangles():\n",
        "    tris = []\n",
        "    def cell_id(r, c):\n",
        "        return r * 4 + c\n",
        "\n",
        "    # Row triangles\n",
        "    for r in range(4):\n",
        "        cells = [cell_id(r, c) for c in range(4)]\n",
        "        for i, j, k in itertools.combinations(cells, 3):\n",
        "            tris.append((i, j, k))\n",
        "\n",
        "    # Column triangles\n",
        "    for c in range(4):\n",
        "        cells = [cell_id(r, c) for r in range(4)]\n",
        "        for i, j, k in itertools.combinations(cells, 3):\n",
        "            tris.append((i, j, k))\n",
        "\n",
        "    # Block triangles (2x2)\n",
        "    blocks = [\n",
        "        [(0, 0), (0, 1), (1, 0), (1, 1)],\n",
        "        [(0, 2), (0, 3), (1, 2), (1, 3)],\n",
        "        [(2, 0), (2, 1), (3, 0), (3, 1)],\n",
        "        [(2, 2), (2, 3), (3, 2), (3, 3)],\n",
        "    ]\n",
        "    for blk in blocks:\n",
        "        cells = [cell_id(r, c) for (r, c) in blk]\n",
        "        for i, j, k in itertools.combinations(cells, 3):\n",
        "            tris.append((i, j, k))\n",
        "\n",
        "    return tris\n",
        "\n",
        "def triangle_consistency(hT: torch.Tensor, triangles, reduce: str = 'mean'):\n",
        "    if hT.ndim == 2:\n",
        "        hT = hT.unsqueeze(0)\n",
        "    B, T, D = hT.shape\n",
        "    total = 0.0\n",
        "    for (i, j, k) in triangles:\n",
        "        vi = hT[:, i, :]\n",
        "        vj = hT[:, j, :]\n",
        "        vk = hT[:, k, :]\n",
        "        mean = (vi + vj + vk) / 3.0\n",
        "        tri_loss = (\n",
        "            (vi - mean).pow(2).sum(-1) +\n",
        "            (vj - mean).pow(2).sum(-1) +\n",
        "            (vk - mean).pow(2).sum(-1)\n",
        "        )\n",
        "        total += tri_loss.mean() if reduce == 'mean' else tri_loss.sum()\n",
        "    return total / max(1, len(triangles))\n",
        "\n",
        "triangles = build_sudoku_triangles()\n",
        "print('Num triangles:', len(triangles))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Training loop\n",
        "We train the baseline and the GT. The DB penalty is optional.\n",
        "You can toggle `lambda_db` from `0.0` to a small value like `0.05`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_sudoku_model(\n",
        "    model,\n",
        "    name,\n",
        "    train_ds,\n",
        "    val_ds,\n",
        "    batch_size=32,\n",
        "    num_epochs=50,\n",
        "    lr=1e-3,\n",
        "    triangles=None,\n",
        "    lambda_db: float = 0.0,\n",
        "):\n",
        "    model = model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    def iterate_batches(dataset, batch_size):\n",
        "        idxs = list(range(len(dataset)))\n",
        "        while True:\n",
        "            random.shuffle(idxs)\n",
        "            for i in range(0, len(idxs), batch_size):\n",
        "                batch_idx = idxs[i:i + batch_size]\n",
        "                puzzles = torch.stack([dataset[j].puzzle for j in batch_idx], dim=0)\n",
        "                sols = torch.stack([dataset[j].solution for j in batch_idx], dim=0)\n",
        "                yield puzzles.to(device), sols.to(device)\n",
        "\n",
        "    train_it = iterate_batches(train_ds, batch_size)\n",
        "    uses_db = triangles is not None and lambda_db > 0.0\n",
        "\n",
        "    logs = {\n",
        "        'epoch': [],\n",
        "        'train_loss': [],\n",
        "        'val_cell_acc': [],\n",
        "        'val_puzzle_acc': [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        steps = 0\n",
        "\n",
        "        for _ in range(len(train_ds) // batch_size):\n",
        "            puzzles, sols = next(train_it)\n",
        "            if uses_db:\n",
        "                logits, hT = model(puzzles, return_embeddings=True)\n",
        "            else:\n",
        "                logits = model(puzzles)\n",
        "                hT = None\n",
        "\n",
        "            ce_loss = F.cross_entropy(logits.view(-1, 4), sols.view(-1))\n",
        "            loss = ce_loss\n",
        "            if uses_db and hT is not None:\n",
        "                curv = triangle_consistency(hT, triangles)\n",
        "                loss = ce_loss + lambda_db * curv\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            steps += 1\n",
        "\n",
        "        avg_loss = running_loss / max(1, steps)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        correct_cells = 0\n",
        "        total_cells = 0\n",
        "        full_correct = 0\n",
        "        total_puzzles = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inst in val_ds:\n",
        "                puzzle = inst.puzzle.to(device)\n",
        "                sol = inst.solution.to(device)\n",
        "                logits = model(puzzle.unsqueeze(0))\n",
        "                if logits.ndim == 3:\n",
        "                    logits = logits.squeeze(0)\n",
        "                pred = logits.argmax(dim=-1)\n",
        "\n",
        "                correct_cells += (pred == sol).sum().item()\n",
        "                total_cells += sol.numel()\n",
        "                if (pred == sol).all():\n",
        "                    full_correct += 1\n",
        "                total_puzzles += 1\n",
        "\n",
        "        val_cell_acc = correct_cells / total_cells\n",
        "        val_puzzle_acc = full_correct / total_puzzles if total_puzzles > 0 else 0.0\n",
        "\n",
        "        logs['epoch'].append(epoch)\n",
        "        logs['train_loss'].append(avg_loss)\n",
        "        logs['val_cell_acc'].append(val_cell_acc)\n",
        "        logs['val_puzzle_acc'].append(val_puzzle_acc)\n",
        "\n",
        "        if epoch % 10 == 0 or epoch == 1:\n",
        "            print(f'[{name}] Epoch {epoch:3d} | loss={avg_loss:.4f} | cell_acc={val_cell_acc:.4f} | puzzle_acc={val_puzzle_acc:.4f}')\n",
        "\n",
        "    return model, logs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Run a small experiment\n",
        "Keep epochs small for quick classroom demos. Increase for stronger results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Dataset\n",
        "num_train = 600\n",
        "num_val = 200\n",
        "num_givens = 8\n",
        "train_ds = make_sudoku_dataset(num_train, num_givens)\n",
        "val_ds = make_sudoku_dataset(num_val, num_givens)\n",
        "\n",
        "# Baseline\n",
        "tf_model = SudokuTransformer(d_model=64, n_heads=4, num_layers=2)\n",
        "tf_model, tf_logs = train_sudoku_model(\n",
        "    tf_model, 'Transformer', train_ds, val_ds,\n",
        "    num_epochs=50, batch_size=32, lr=1e-3\n",
        ")\n",
        "\n",
        "# GT + optional DB\n",
        "gt_model = SudokuGT(d_model=64, n_heads=4, num_layers=2)\n",
        "gt_model, gt_logs = train_sudoku_model(\n",
        "    gt_model, 'GT', train_ds, val_ds,\n",
        "    num_epochs=50, batch_size=32, lr=1e-3,\n",
        "    triangles=triangles, lambda_db=0.0  # try 0.05\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Plot validation accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(tf_logs['epoch'], tf_logs['val_cell_acc'], label='Transformer')\n",
        "plt.plot(gt_logs['epoch'], gt_logs['val_cell_acc'], label='GT (+DB optional)')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Validation cell accuracy')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Inspect a single puzzle\n",
        "This is just for intuition: we compare a masked puzzle and the model\u2019s prediction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def format_grid(vec16):\n",
        "    rows = [vec16[i:i+4] for i in range(0, 16, 4)]\n",
        "    return '\\n'.join([' '.join([str(x) for x in row]) for row in rows])\n",
        "\n",
        "sample = val_ds[0]\n",
        "puzzle = sample.puzzle.to(device)\n",
        "sol = sample.solution\n",
        "\n",
        "with torch.no_grad():\n",
        "    logits = gt_model(puzzle.unsqueeze(0))\n",
        "    pred = logits.squeeze(0).argmax(dim=-1).cpu()\n",
        "\n",
        "print('Puzzle (-1 = blank):')\n",
        "print(format_grid(puzzle.cpu().tolist()))\n",
        "print('Prediction:')\n",
        "print(format_grid(pred.tolist()))\n",
        "print('Solution:')\n",
        "print(format_grid(sol.tolist()))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Sudoku_GT_DB_Colab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}