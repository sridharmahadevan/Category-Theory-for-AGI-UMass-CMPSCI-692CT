{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 \u2014 Language Modeling: GT vs Transformer (PTB)\n",
        "This notebook compares a **baseline Transformer** to a **Geometric Transformer (GT-Lite)** on a small language modeling task.\n",
        "We use a tiny **Penn Treebank (PTB)** word-level dataset and train for a short run to keep things fast.\n",
        "\n",
        "**Goal:** show how GT-style local smoothing can change learning dynamics relative to a standard Transformer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core imports\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from urllib import request\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(0)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Load PTB (word-level)\n",
        "We download a tiny PTB word-level dataset and build a vocabulary from the training split.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def download_ptb(save_dir='/tmp/ptb'):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    base = 'https://raw.githubusercontent.com/wojzaremba/lstm/master/data/'\n",
        "    files = ['ptb.train.txt', 'ptb.valid.txt', 'ptb.test.txt']\n",
        "    for fn in files:\n",
        "        path = os.path.join(save_dir, fn)\n",
        "        if not os.path.exists(path):\n",
        "            request.urlretrieve(base + fn, path)\n",
        "    return [os.path.join(save_dir, f) for f in files]\n",
        "\n",
        "def load_tokens(path):\n",
        "    with open(path, 'r') as f:\n",
        "        lines = [line.strip().split() for line in f]\n",
        "    # add <eos> at end of each line\n",
        "    return [tok for line in lines for tok in (line + ['<eos>'])]\n",
        "\n",
        "train_path, valid_path, test_path = download_ptb()\n",
        "train_tokens = load_tokens(train_path)\n",
        "valid_tokens = load_tokens(valid_path)\n",
        "test_tokens  = load_tokens(test_path)\n",
        "\n",
        "vocab = {w:i for i,w in enumerate(sorted(set(train_tokens)))}\n",
        "\n",
        "def to_ids(tokens):\n",
        "    return torch.tensor([vocab[w] for w in tokens], dtype=torch.long)\n",
        "\n",
        "train_ids = to_ids(train_tokens)\n",
        "valid_ids = to_ids(valid_tokens)\n",
        "test_ids  = to_ids(test_tokens)\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('Vocab size:', vocab_size)\n",
        "print('Train tokens:', len(train_ids))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Batching utilities\n",
        "We use the classic PTB batching scheme (contiguous segments).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batchify(data, batch_size):\n",
        "    n_batch = data.size(0) // batch_size\n",
        "    data = data[:n_batch * batch_size]\n",
        "    data = data.view(batch_size, -1).t().contiguous()  # (T, B)\n",
        "    return data\n",
        "\n",
        "def get_batch(source, i, bptt=35):\n",
        "    seq_len = min(bptt, source.size(0) - 1 - i)\n",
        "    data = source[i:i+seq_len]       # (seq_len, B)\n",
        "    target = source[i+1:i+1+seq_len] # (seq_len, B)\n",
        "    return data, target\n",
        "\n",
        "batch_size = 20\n",
        "bptt = 35\n",
        "train_data = batchify(train_ids, batch_size)\n",
        "valid_data = batchify(valid_ids, batch_size)\n",
        "test_data  = batchify(test_ids,  batch_size)\n",
        "\n",
        "print('Train batchified shape:', train_data.shape)  # (T, B)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Models: Transformer vs GT-Lite\n",
        "GT-Lite adds a small 1D convolution in each block to encourage local smoothing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pos = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(pos * div)\n",
        "        pe[:, 1::2] = torch.cos(pos * div)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0)].unsqueeze(1)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dim_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.ln1 = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_ff, d_model),\n",
        "        )\n",
        "        self.ln2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        # x: (T, B, D)\n",
        "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
        "        x = self.ln1(x + self.dropout(attn_out))\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln2(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class GeomTransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model, n_heads, dim_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout)\n",
        "        self.ln_attn = nn.LayerNorm(d_model)\n",
        "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=1)\n",
        "        self.ln_conv = nn.LayerNorm(d_model)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_model, dim_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(dim_ff, d_model),\n",
        "        )\n",
        "        self.ln_ff = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
        "        x = self.ln_attn(x + self.dropout(attn_out))\n",
        "\n",
        "        x_conv = self.conv(x.permute(1, 2, 0)).permute(2, 0, 1)  # (T,B,D)\n",
        "        x = self.ln_conv(x + 0.2 * x_conv)\n",
        "\n",
        "        ff_out = self.ff(x)\n",
        "        x = self.ln_ff(x + self.dropout(ff_out))\n",
        "        return x\n",
        "\n",
        "class LMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, n_heads=4, n_layers=2, dim_ff=512, gt=False):\n",
        "        super().__init__()\n",
        "        self.tok = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = PositionalEncoding(d_model)\n",
        "        block = GeomTransformerBlock if gt else TransformerBlock\n",
        "        self.layers = nn.ModuleList([block(d_model, n_heads, dim_ff) for _ in range(n_layers)])\n",
        "        self.ln = nn.LayerNorm(d_model)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def _causal_mask(self, T, device):\n",
        "        mask = torch.triu(torch.full((T, T), float('-inf'), device=device), diagonal=1)\n",
        "        return mask\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (T, B)\n",
        "        emb = self.tok(x)  # (T, B, D)\n",
        "        h = self.pos(emb)\n",
        "        mask = self._causal_mask(x.size(0), x.device)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, attn_mask=mask)\n",
        "        h = self.ln(h)\n",
        "        logits = self.out(h)\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Training + evaluation\n",
        "We keep it short for classroom use. Increase `max_steps` for stronger results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate(model, data_source, bptt=35):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_tokens = 0\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, data_source.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(data_source, i, bptt)\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "            logits = model(data)\n",
        "            # logits: (T,B,V) -> (T*B,V)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.reshape(-1))\n",
        "            total_loss += loss.item() * targets.numel()\n",
        "            total_tokens += targets.numel()\n",
        "    avg_loss = total_loss / total_tokens\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl\n",
        "\n",
        "def train_model(model, name, train_data, valid_data, max_steps=300, bptt=35, lr=1e-3):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    losses = []\n",
        "    val_ppl = []\n",
        "    step = 0\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(1000):\n",
        "        for i in range(0, train_data.size(0) - 1, bptt):\n",
        "            data, targets = get_batch(train_data, i, bptt)\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            logits = model(data)\n",
        "            loss = F.cross_entropy(logits.view(-1, vocab_size), targets.reshape(-1))\n",
        "\n",
        "            opt.zero_grad()\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            step += 1\n",
        "            if step % 50 == 0:\n",
        "                losses.append(loss.item())\n",
        "\n",
        "            if step % 100 == 0:\n",
        "                v_loss, v_ppl = evaluate(model, valid_data, bptt)\n",
        "                val_ppl.append(v_ppl)\n",
        "                print(f'[{name}] step {step:4d} | train_loss={loss.item():.3f} | val_ppl={v_ppl:.2f}')\n",
        "\n",
        "            if step >= max_steps:\n",
        "                return losses, val_ppl\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Run GT vs Transformer\n",
        "Try 300 steps for a fast demo; increase to 1000+ for stronger separation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "d_model = 128\n",
        "n_heads = 4\n",
        "n_layers = 2\n",
        "dim_ff = 512\n",
        "max_steps = 300\n",
        "\n",
        "# Baseline Transformer\n",
        "tf_model = LMModel(vocab_size, d_model, n_heads, n_layers, dim_ff, gt=False)\n",
        "tf_losses, tf_ppl = train_model(tf_model, 'Transformer', train_data, valid_data, max_steps=max_steps, bptt=bptt)\n",
        "\n",
        "# Geometric Transformer (GT-Lite)\n",
        "gt_model = LMModel(vocab_size, d_model, n_heads, n_layers, dim_ff, gt=True)\n",
        "gt_losses, gt_ppl = train_model(gt_model, 'GT', train_data, valid_data, max_steps=max_steps, bptt=bptt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Plot validation perplexity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(tf_ppl, label='Transformer')\n",
        "plt.plot(gt_ppl, label='GT-Lite')\n",
        "plt.xlabel('Eval checkpoints')\n",
        "plt.ylabel('Validation perplexity')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Quick text generation (greedy)\n",
        "We sample a short continuation from each model for intuition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build inverse vocab for decoding\n",
        "inv_vocab = {i:w for w,i in vocab.items()}\n",
        "\n",
        "def greedy_generate(model, start_tokens, max_new=25):\n",
        "    model.eval()\n",
        "    seq = start_tokens.clone()  # (T, B)\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_new):\n",
        "            logits = model(seq)  # (T,B,V)\n",
        "            next_token = logits[-1].argmax(dim=-1, keepdim=True)  # (B,1)\n",
        "            seq = torch.cat([seq, next_token.t()], dim=0)  # (T+1,B)\n",
        "    return seq\n",
        "\n",
        "# Use a short prefix from validation data\n",
        "prefix_len = 10\n",
        "start = valid_data[:prefix_len].to(device)  # (T,B)\n",
        "\n",
        "tf_seq = greedy_generate(tf_model, start, max_new=20).cpu()\n",
        "gt_seq = greedy_generate(gt_model, start, max_new=20).cpu()\n",
        "\n",
        "def decode(seq, col=0):\n",
        "    toks = [inv_vocab[int(t)] for t in seq[:, col]]\n",
        "    return ' '.join(toks)\n",
        "\n",
        "print('Prefix:')\n",
        "print(decode(start.cpu(), col=0))\n",
        "print('\\nTransformer continuation:')\n",
        "print(decode(tf_seq, col=0))\n",
        "print('\\nGT continuation:')\n",
        "print(decode(gt_seq, col=0))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "week01_lm_gt_vs_transformer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}