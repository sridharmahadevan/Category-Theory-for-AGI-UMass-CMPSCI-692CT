{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 1 \u2014 Causal Leakage Check: Transformer vs GT\n",
        "\n",
        "This notebook tests a specific question: does GT get unfair next-token information from a non-causal geometric mixing layer?\n",
        "\n",
        "We run a synthetic stress test where true next tokens are independent random symbols.\n",
        "- A strictly causal model should stay near chance.\n",
        "- A model with future-token leakage can artificially achieve high accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "set_seed(0)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TinyTransformerLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=1, max_len=128):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = nn.Embedding(max_len, d_model)\n",
        "        enc = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=4*d_model, batch_first=True)\n",
        "        self.tr = nn.TransformerEncoder(enc, num_layers=num_layers)\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        p = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
        "        h = self.emb(x) + self.pos(p)\n",
        "        mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "        h = self.tr(h, mask)\n",
        "        return self.out(h)\n",
        "\n",
        "class TinyGTBlock(nn.Module):\n",
        "    def __init__(self, d_model=64, nhead=4, geo_causal=False):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "        self.ffn = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.GELU(), nn.Linear(4*d_model, d_model))\n",
        "        self.geo_causal = bool(geo_causal)\n",
        "        self.conv = nn.Conv1d(d_model, d_model, kernel_size=3, padding=0 if geo_causal else 1, groups=d_model)\n",
        "        self.n1 = nn.LayerNorm(d_model)\n",
        "        self.n2 = nn.LayerNorm(d_model)\n",
        "        self.ng = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, attn_mask=None):\n",
        "        y, _ = self.attn(x, x, x, attn_mask=attn_mask, need_weights=False)\n",
        "        x = self.n1(x + y)\n",
        "        x = self.n2(x + self.ffn(x))\n",
        "        z = x.transpose(1, 2)\n",
        "        if self.geo_causal:\n",
        "            z = F.pad(z, (2, 0))\n",
        "        z = self.conv(z).transpose(1, 2)\n",
        "        return self.ng(x + z)\n",
        "\n",
        "class TinyGTLM(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=64, nhead=4, num_layers=1, max_len=128, geo_causal=False):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos = nn.Embedding(max_len, d_model)\n",
        "        self.layers = nn.ModuleList([TinyGTBlock(d_model=d_model, nhead=nhead, geo_causal=geo_causal) for _ in range(num_layers)])\n",
        "        self.out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, L = x.shape\n",
        "        p = torch.arange(L, device=x.device).unsqueeze(0).expand(B, L)\n",
        "        h = self.emb(x) + self.pos(p)\n",
        "        mask = torch.triu(torch.ones(L, L, device=x.device, dtype=torch.bool), diagonal=1)\n",
        "        for layer in self.layers:\n",
        "            h = layer(h, attn_mask=mask)\n",
        "        return self.out(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def future_sensitivity(model, vocab_size=50, seq_len=24, batch=64, probe_t=8, trials=20):\n",
        "    model.eval()\n",
        "    diffs = []\n",
        "    changed = 0\n",
        "    for _ in range(trials):\n",
        "        x = torch.randint(0, vocab_size, (batch, seq_len), device=device)\n",
        "        x2 = x.clone()\n",
        "        x2[:, probe_t + 1] = torch.randint(0, vocab_size, (batch,), device=device)\n",
        "\n",
        "        y1 = model(x)[:, probe_t, :]\n",
        "        y2 = model(x2)[:, probe_t, :]\n",
        "        d = (y1 - y2).abs().mean(dim=1)\n",
        "        diffs.append(d.mean().item())\n",
        "        changed += (d > 1e-8).float().sum().item()\n",
        "\n",
        "    return {\n",
        "        'mean_abs_logit_delta': float(sum(diffs) / len(diffs)),\n",
        "        'fraction_changed': float(changed / (trials * batch)),\n",
        "    }\n",
        "\n",
        "VOCAB = 64\n",
        "cfg = dict(vocab_size=VOCAB, d_model=64, nhead=4, num_layers=1, max_len=64)\n",
        "m_tf = TinyTransformerLM(**cfg).to(device)\n",
        "m_gt_nc = TinyGTLM(**cfg, geo_causal=False).to(device)\n",
        "m_gt_c = TinyGTLM(**cfg, geo_causal=True).to(device)\n",
        "\n",
        "sens = {\n",
        "    'transformer_causal': future_sensitivity(m_tf, vocab_size=VOCAB),\n",
        "    'gt_noncausal_conv': future_sensitivity(m_gt_nc, vocab_size=VOCAB),\n",
        "    'gt_causal_conv': future_sensitivity(m_gt_c, vocab_size=VOCAB),\n",
        "}\n",
        "sens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sample_batch(batch_size=64, seq_len=25, vocab_size=64):\n",
        "    s = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
        "    x = s[:, :-1]  # observed tokens\n",
        "    y = s[:, 1:]   # next tokens (independent random)\n",
        "    return x, y\n",
        "\n",
        "def run_train(model, steps=250, lr=3e-4, batch_size=64, seq_len=25, vocab_size=64):\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    hist_loss, hist_acc = [], []\n",
        "    model.train()\n",
        "    for _ in range(steps):\n",
        "        x, y = sample_batch(batch_size=batch_size, seq_len=seq_len, vocab_size=vocab_size)\n",
        "        logits = model(x)\n",
        "        loss = ce(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "        opt.zero_grad()\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            acc = (pred == y).float().mean().item()\n",
        "        hist_loss.append(loss.item())\n",
        "        hist_acc.append(acc)\n",
        "    return hist_loss, hist_acc\n",
        "\n",
        "def eval_model(model, batches=40, batch_size=64, seq_len=25, vocab_size=64):\n",
        "    ce = nn.CrossEntropyLoss()\n",
        "    model.eval()\n",
        "    losses, accs = [], []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(batches):\n",
        "            x, y = sample_batch(batch_size=batch_size, seq_len=seq_len, vocab_size=vocab_size)\n",
        "            logits = model(x)\n",
        "            loss = ce(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
        "            pred = logits.argmax(dim=-1)\n",
        "            acc = (pred == y).float().mean().item()\n",
        "            losses.append(loss.item())\n",
        "            accs.append(acc)\n",
        "    return float(sum(losses)/len(losses)), float(sum(accs)/len(accs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "set_seed(0)\n",
        "cfg = dict(vocab_size=64, d_model=64, nhead=4, num_layers=1, max_len=64)\n",
        "models = {\n",
        "    'transformer_causal': TinyTransformerLM(**cfg).to(device),\n",
        "    'gt_noncausal_conv': TinyGTLM(**cfg, geo_causal=False).to(device),\n",
        "    'gt_causal_conv': TinyGTLM(**cfg, geo_causal=True).to(device),\n",
        "}\n",
        "\n",
        "hist = {}\n",
        "for name, m in models.items():\n",
        "    l, a = run_train(m, steps=250, lr=3e-4, batch_size=64, seq_len=25, vocab_size=64)\n",
        "    te_loss, te_acc = eval_model(m, batches=50, batch_size=64, seq_len=25, vocab_size=64)\n",
        "    hist[name] = {'loss': l, 'acc': a, 'test_loss': te_loss, 'test_acc': te_acc}\n",
        "\n",
        "chance_acc = 1.0 / 64.0\n",
        "chance_ce = math.log(64.0)\n",
        "print('chance acc =', round(chance_acc, 4), 'chance CE =', round(chance_ce, 4))\n",
        "for name in hist:\n",
        "    print(name, 'test_loss=', round(hist[name]['test_loss'], 4), 'test_acc=', round(hist[name]['test_acc'], 4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "for name, d in hist.items():\n",
        "    ax[0].plot(d['loss'], label=name)\n",
        "    ax[1].plot(d['acc'], label=name)\n",
        "ax[0].axhline(math.log(64.0), color='k', ls='--', alpha=0.5, label='chance CE')\n",
        "ax[1].axhline(1.0/64.0, color='k', ls='--', alpha=0.5, label='chance acc')\n",
        "ax[0].set_title('Train CE loss')\n",
        "ax[1].set_title('Train token accuracy')\n",
        "for a in ax:\n",
        "    a.set_xlabel('step')\n",
        "    a.grid(alpha=0.3)\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "\n",
        "If `gt_noncausal_conv` is much better than chance on this random-token task while causal models remain near chance, the gain is from future-token leakage, not better sequence modeling.\n",
        "\n",
        "For fair LM comparisons, use strictly causal geometric mixing (`geo_causal=True`) so the GT layer respects the same information boundary as baseline Transformers.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}