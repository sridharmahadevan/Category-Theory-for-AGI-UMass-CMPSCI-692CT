{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Week 11 - Coalgebraic RL with GT + Diagrammatic Backprop (DB)\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sridharmahadevan/Category-Theory-for-AGI-UMass-CMPSCI-692CT/blob/main/notebooks/week11_coalgebra_gtdb_rl_demo.ipynb)\n",
        "\n",
        "This notebook is a compact experiment-first demo for **coalgebras and coinduction** in RL:\n",
        "\n",
        "- Environment as a coalgebra-like transition structure over states\n",
        "- Three learning arms: **MLP**, **GT**, **GT+DB**\n",
        "- Harder synthetic MDP (random starts, stochastic slips, reward noise)\n",
        "- Compare sample efficiency and final success\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Environment\n",
        "import random\n",
        "from dataclasses import dataclass\n",
        "from statistics import mean\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@dataclass(frozen=True)\n",
        "class MDPConfig:\n",
        "    n_states: int = 32\n",
        "    start_state: int = 0\n",
        "    goal_state: int = 31\n",
        "    max_steps: int = 36\n",
        "    step_penalty: float = -0.01\n",
        "    goal_reward: float = 1.0\n",
        "    random_start: bool = True\n",
        "    slip_prob: float = 0.10\n",
        "    reward_noise_std: float = 0.05\n",
        "\n",
        "class LineWorldMDP:\n",
        "    def __init__(self, cfg: MDPConfig):\n",
        "        self.cfg = cfg\n",
        "        self.state = cfg.start_state\n",
        "        self.steps = 0\n",
        "\n",
        "    def reset(self):\n",
        "        self.steps = 0\n",
        "        if self.cfg.random_start:\n",
        "            self.state = random.randrange(self.cfg.n_states)\n",
        "        else:\n",
        "            self.state = self.cfg.start_state\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        self.steps += 1\n",
        "        a = int(action)\n",
        "        if random.random() < self.cfg.slip_prob:\n",
        "            a = 1 - a\n",
        "\n",
        "        if a == 0:\n",
        "            ns = max(0, self.state - 1)\n",
        "        else:\n",
        "            ns = min(self.cfg.n_states - 1, self.state + 1)\n",
        "\n",
        "        r = self.cfg.step_penalty\n",
        "        done = False\n",
        "        if ns == self.cfg.goal_state:\n",
        "            r = self.cfg.goal_reward\n",
        "            done = True\n",
        "        if self.steps >= self.cfg.max_steps:\n",
        "            done = True\n",
        "\n",
        "        r += random.gauss(0.0, self.cfg.reward_noise_std)\n",
        "        self.state = ns\n",
        "        return ns, float(r), bool(done), {'is_goal': int(ns == self.cfg.goal_state)}\n",
        "\n",
        "def build_transition_relation(n_states):\n",
        "    rel = torch.zeros((n_states, n_states), dtype=torch.float32)\n",
        "    for s in range(n_states):\n",
        "        l = max(0, s - 1)\n",
        "        r = min(n_states - 1, s + 1)\n",
        "        if l != s:\n",
        "            rel[s, l] = 1.0\n",
        "        if r != s:\n",
        "            rel[s, r] = 1.0\n",
        "    return rel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GTQ(nn.Module):\n",
        "    def __init__(self, n_states, n_actions=2, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.n_states = n_states\n",
        "        self.emb = nn.Embedding(n_states, hidden_dim)\n",
        "        self.msg = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.head = nn.Linear(hidden_dim, n_actions)\n",
        "\n",
        "    def q_values(self, state, rel):\n",
        "        idx = torch.arange(self.n_states, device=rel.device)\n",
        "        h = self.emb(idx)\n",
        "        deg = rel.sum(dim=1, keepdim=True).clamp(min=1.0)\n",
        "        agg = (rel @ h) / deg\n",
        "        h2 = F.relu(h + self.msg(agg))\n",
        "        q = self.head(h2)\n",
        "        return q[state], h2\n",
        "\n",
        "    def db_loss(self, h_all, rel):\n",
        "        src, dst = torch.where(rel > 0.0)\n",
        "        if src.numel() == 0:\n",
        "            return h_all.new_zeros(())\n",
        "        d = h_all[src] - h_all[dst]\n",
        "        return (d.pow(2).sum(dim=1)).mean()\n",
        "\n",
        "class MLPQ(nn.Module):\n",
        "    def __init__(self, n_states, n_actions=2, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.n_states = n_states\n",
        "        self.fc1 = nn.Linear(n_states, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, n_actions)\n",
        "\n",
        "    def q_values(self, state, rel):\n",
        "        _ = rel\n",
        "        x = torch.zeros((1, self.n_states), device=self.fc1.weight.device)\n",
        "        x[0, state] = 1.0\n",
        "        h = F.relu(self.fc1(x))\n",
        "        q = self.fc2(h).squeeze(0)\n",
        "        return q, h.repeat(self.n_states, 1)\n",
        "\n",
        "    def db_loss(self, h_all, rel):\n",
        "        _ = h_all\n",
        "        _ = rel\n",
        "        return torch.tensor(0.0, device=self.fc1.weight.device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one(model_name='gtdb', seed=0, episodes=180, gamma=0.99, lr=1e-3, eps0=0.2, eps_min=0.02, eps_decay=0.995, db_coef=0.1):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    cfg = MDPConfig()\n",
        "    env = LineWorldMDP(cfg)\n",
        "    rel = build_transition_relation(cfg.n_states).to(device)\n",
        "\n",
        "    if model_name == 'mlp':\n",
        "        model = MLPQ(cfg.n_states).to(device)\n",
        "        db_coef_eff = 0.0\n",
        "    else:\n",
        "        model = GTQ(cfg.n_states).to(device)\n",
        "        db_coef_eff = db_coef if model_name == 'gtdb' else 0.0\n",
        "\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    ep_returns, ep_success, ep_td, ep_db = [], [], [], []\n",
        "    eps = eps0\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        s = env.reset()\n",
        "        ret = 0.0\n",
        "        td_hist, db_hist = [], []\n",
        "        got_goal = 0\n",
        "\n",
        "        for _ in range(cfg.max_steps):\n",
        "            q_s, h_all = model.q_values(s, rel)\n",
        "            if random.random() < eps:\n",
        "                a = random.choice([0, 1])\n",
        "            else:\n",
        "                a = int(torch.argmax(q_s).item())\n",
        "\n",
        "            ns, r, done, info = env.step(a)\n",
        "            q_sa = q_s[a]\n",
        "\n",
        "            with torch.no_grad():\n",
        "                q_ns, _ = model.q_values(ns, rel)\n",
        "                target = torch.tensor(r, device=device) + (0.0 if done else gamma * torch.max(q_ns))\n",
        "\n",
        "            td = F.mse_loss(q_sa, target)\n",
        "            db = model.db_loss(h_all, rel)\n",
        "            loss = td + db_coef_eff * db\n",
        "\n",
        "            opt.zero_grad(set_to_none=True)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "            td_hist.append(float(td.item()))\n",
        "            db_hist.append(float(db.item()))\n",
        "            ret += r\n",
        "            s = ns\n",
        "            got_goal = max(got_goal, int(info['is_goal']))\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        ep_returns.append(ret)\n",
        "        ep_success.append(float(got_goal))\n",
        "        ep_td.append(mean(td_hist) if td_hist else 0.0)\n",
        "        ep_db.append(mean(db_hist) if db_hist else 0.0)\n",
        "        eps = max(eps_min, eps * eps_decay)\n",
        "\n",
        "    return {\n",
        "        'model': model_name,\n",
        "        'seed': seed,\n",
        "        'return': ep_returns,\n",
        "        'success': ep_success,\n",
        "        'td': ep_td,\n",
        "        'db': ep_db,\n",
        "    }\n",
        "\n",
        "def moving_avg(xs, w=20):\n",
        "    out, buf, s = [], [], 0.0\n",
        "    for x in xs:\n",
        "        buf.append(x); s += x\n",
        "        if len(buf) > w:\n",
        "            s -= buf.pop(0)\n",
        "        out.append(s / len(buf))\n",
        "    return out\n",
        "\n",
        "def episodes_to_target(success_series, target=0.8, window=20):\n",
        "    ma = moving_avg(success_series, window)\n",
        "    for i, v in enumerate(ma):\n",
        "        if v >= target:\n",
        "            return i + 1\n",
        "    return len(success_series) + 1\n",
        "\n",
        "def auc(y):\n",
        "    if len(y) < 2:\n",
        "        return 0.0\n",
        "    return float(sum(0.5 * (y[i-1] + y[i]) for i in range(1, len(y))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run a compact ablation\n",
        "MODELS = ['gtdb', 'gt', 'mlp']\n",
        "SEEDS = list(range(6))\n",
        "EPISODES = 180\n",
        "\n",
        "runs = []\n",
        "for m in MODELS:\n",
        "    for s in SEEDS:\n",
        "        runs.append(train_one(model_name=m, seed=s, episodes=EPISODES))\n",
        "\n",
        "print('completed runs:', len(runs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate metrics and simple table\n",
        "summary = {}\n",
        "for m in MODELS:\n",
        "    rs = [r for r in runs if r['model'] == m]\n",
        "    auc_vals = [auc(r['return']) for r in rs]\n",
        "    final_success_vals = [mean(r['success'][-max(1, int(0.2*len(r['success']))):]) for r in rs]\n",
        "    ett_vals = [episodes_to_target(r['success'], target=0.8, window=20) for r in rs]\n",
        "    reached = [1.0 if v <= EPISODES else 0.0 for v in ett_vals]\n",
        "    summary[m] = {\n",
        "        'auc_mean': mean(auc_vals),\n",
        "        'final_success_mean': mean(final_success_vals),\n",
        "        'episodes_to_target_mean': mean(ett_vals),\n",
        "        'reached_target_rate': mean(reached),\n",
        "    }\n",
        "\n",
        "for m in MODELS:\n",
        "    s = summary[m]\n",
        "    print(f\"{m:>4} | AUC={s['auc_mean']:.2f} | final_success={s['final_success_mean']:.3f} | ETT={s['episodes_to_target_mean']:.1f} | reached={s['reached_target_rate']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot return and success convergence\n",
        "x = np.arange(1, EPISODES + 1)\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "for m in MODELS:\n",
        "    rs = [r for r in runs if r['model'] == m]\n",
        "    ret_stack = np.array([moving_avg(r['return'], 20) for r in rs])\n",
        "    suc_stack = np.array([moving_avg(r['success'], 20) for r in rs])\n",
        "\n",
        "    ret_mean, ret_std = ret_stack.mean(axis=0), ret_stack.std(axis=0)\n",
        "    suc_mean, suc_std = suc_stack.mean(axis=0), suc_stack.std(axis=0)\n",
        "\n",
        "    axes[0].plot(x, ret_mean, label=m)\n",
        "    axes[0].fill_between(x, ret_mean-ret_std, ret_mean+ret_std, alpha=0.15)\n",
        "\n",
        "    axes[1].plot(x, suc_mean, label=m)\n",
        "    axes[1].fill_between(x, suc_mean-suc_std, suc_mean+suc_std, alpha=0.15)\n",
        "\n",
        "axes[0].set_title('Rolling Return (window=20)')\n",
        "axes[0].set_xlabel('Episode'); axes[0].set_ylabel('Return')\n",
        "axes[0].grid(True, alpha=0.3); axes[0].legend()\n",
        "\n",
        "axes[1].set_title('Rolling Success Rate (window=20)')\n",
        "axes[1].set_xlabel('Episode'); axes[1].set_ylabel('Success')\n",
        "axes[1].set_ylim(-0.05, 1.05)\n",
        "axes[1].grid(True, alpha=0.3); axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interpretation\n",
        "\n",
        "If **GT+DB** improves episodes-to-target or AUC under this harder stochastic setting, the explanation is language-level: DB enforces invariants from declared transition structure, reducing geometry-inconsistent solutions during RL optimization.\n",
        "\n",
        "This is the key course message for coalgebras/coinduction in practical ML:\n",
        "\n",
        "- coalgebra gives the behavior type,\n",
        "- GT gives representation over that structure,\n",
        "- DB makes the optimization respect the declared diagram.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
