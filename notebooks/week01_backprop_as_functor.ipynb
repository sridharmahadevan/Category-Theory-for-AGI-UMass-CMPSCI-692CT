{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3725bcb6",
   "metadata": {},
   "source": [
    "# Week 1 — Backprop as a Functor (Micro‑Demo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300dd31",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/sridharmahadevan/Category-Theory-for-AGI-UMass-CMPSCI-692CT/blob/main/notebooks/week01_backprop_as_functor.ipynb)\n",
    "<br/>\n",
    "_Replace `sridharmahadevan/Category-Theory-for-AGI-UMass-CMPSCI-692CT` above once you push this repo to GitHub._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f86231",
   "metadata": {},
   "source": [
    "### Environment (run first)\n",
    "This pins a minimal, stable stack. GPU is **optional**; notebooks run on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55db6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Core scientific stack + causal / graph tooling\n",
    "%pip install -q numpy==1.* pandas==2.* matplotlib==3.* networkx==3.* pgmpy==0.1.* graphviz==0.20.*\n",
    "# Torch CPU by default (Colab often preinstalls a GPU build; this is a safe fallback)\n",
    "%pip install -q torch --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e26f76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install system graphviz only if available (Colab & many Linux envs). Safe to skip elsewhere.\n",
    "!command -v apt-get >/dev/null && apt-get -y -qq install graphviz || echo \"apt-get not available; skipping system graphviz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873089f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform, sys\n",
    "print(\"Python:\", platform.python_version())\n",
    "try:\n",
    "    import torch\n",
    "    print(\"Torch:\", torch.__version__, \"| CUDA available?\", torch.cuda.is_available())\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "except Exception as e:\n",
    "    print(\"Torch not installed, proceeding CPU-only.\")\n",
    "    device = \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01391485",
   "metadata": {},
   "source": [
    "\n",
    "## Learning goals\n",
    "- See how a **computation graph** (objects: layers; morphisms: compositions) is mapped to **compositional learners**.\n",
    "- Observe that composition is preserved: learning `(g ∘ f)` behaves like composing learners for `f` and `g` (chain rule).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ceb6f8b",
   "metadata": {},
   "source": [
    "## 10‑line micro‑demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f0895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch, math\n",
    "torch.manual_seed(0)\n",
    "# Two arrows in Graph: x --f--> h --g--> yhat   (ReLU in the middle)\n",
    "f = torch.nn.Linear(2, 3, bias=False)\n",
    "g = torch.nn.Linear(3, 1, bias=False)\n",
    "relu = torch.nn.ReLU()\n",
    "\n",
    "x = torch.tensor([[1.0, -1.0]])\n",
    "y = torch.tensor([[0.5]])\n",
    "\n",
    "def forward(x):            # (g ∘ ReLU ∘ f)\n",
    "    return g(relu(f(x)))\n",
    "\n",
    "opt = torch.optim.SGD(list(f.parameters()) + list(g.parameters()), lr=0.1)\n",
    "\n",
    "yhat = forward(x)\n",
    "loss = ((yhat - y)**2).mean()\n",
    "opt.zero_grad(); loss.backward(); opt.step()\n",
    "\n",
    "float(loss.detach())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55f75db",
   "metadata": {},
   "source": [
    "\n",
    "### Key observation\n",
    "The gradient on `f` is scaled by the **pushforward** through `g ∘ ReLU` (i.e., chain rule). That is the hallmark of functoriality: composition in the graph corresponds to composition in the learner category.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46aeb7",
   "metadata": {},
   "source": [
    "## Worked example: compare composing before vs. after a step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4235d5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We'll compare a single step update when seen as (g∘f) vs. sequential updates.\n",
    "torch.manual_seed(42)\n",
    "f1 = torch.nn.Linear(2, 3, bias=False); g1 = torch.nn.Linear(3, 1, bias=False); relu = torch.nn.ReLU()\n",
    "f2 = torch.nn.Linear(2, 3, bias=False); g2 = torch.nn.Linear(3, 1, bias=False)\n",
    "f2.weight.data[:] = f1.weight.data; g2.weight.data[:] = g1.weight.data\n",
    "\n",
    "x = torch.tensor([[0.2, 0.8]]); y = torch.tensor([[0.7]])\n",
    "def F(x, f, g): return g(relu(f(x)))\n",
    "\n",
    "def step_pair(f, g, x, y, lr=0.1):\n",
    "    opt = torch.optim.SGD(list(f.parameters())+list(g.parameters()), lr=lr)\n",
    "    yhat = F(x, f, g); loss = ((yhat - y)**2).mean()\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    return float(loss.detach()), f.weight.detach().clone(), g.weight.detach().clone()\n",
    "\n",
    "# Joint step (g∘f)\n",
    "L_joint, fW_joint, gW_joint = step_pair(f1, g1, x, y)\n",
    "\n",
    "# Sequential view (still one joint step, but just to see parameter movement relation)\n",
    "L_seq, fW_seq, gW_seq = step_pair(f2, g2, x, y)\n",
    "\n",
    "print(\"Losses close?\", abs(L_joint - L_seq) < 1e-6)\n",
    "print(\"Δf weight norm:\", float((fW_joint - fW_seq).abs().sum()))\n",
    "print(\"Δg weight norm:\", float((gW_joint - gW_seq).abs().sum()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73944b44",
   "metadata": {},
   "source": [
    "## Exercises (ungraded, quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4438570",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1) Replace ReLU with Tanh and observe the effect on the gradient flow (chain rule factor).\n",
    "# 2) Change the loss to MAE and repeat.\n",
    "# 3) Add a skip connection h' = ReLU(f(x)) + x (shape match) and observe composition changes.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}